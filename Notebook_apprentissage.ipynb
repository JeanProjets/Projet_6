{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0156ea07",
   "metadata": {},
   "source": [
    "# Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330a3ce",
   "metadata": {},
   "source": [
    "## Stemming, lemming, tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503a5548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/j/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/j/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/j/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/j/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The runners were running quickly through the beautiful gardens, carrying heavy boxes.\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. TOKENIZATION:\n",
      "['the', 'runners', 'were', 'running', 'quickly', 'through', 'the', 'beautiful', 'gardens', ',', 'carrying', 'heavy', 'boxes', '.']\n",
      "\n",
      "============================================================\n",
      "\n",
      "2a. STEMMING (Porter Stemmer):\n",
      "['the', 'runner', 'were', 'run', 'quickli', 'through', 'the', 'beauti', 'garden', ',', 'carri', 'heavi', 'box', '.']\n",
      "\n",
      "Examples:\n",
      "  running → run\n",
      "  runner → runner\n",
      "  beautiful → beauti\n",
      "\n",
      "============================================================\n",
      "\n",
      "2b. LEMMATIZATION:\n",
      "['the', 'runners', 'be', 'run', 'quickly', 'through', 'the', 'beautiful', 'garden', ',', 'carry', 'heavy', 'box', '.']\n",
      "\n",
      "Examples:\n",
      "  running → run\n",
      "  runner → runner\n",
      "  beautiful → beautiful\n",
      "\n",
      "============================================================\n",
      "\n",
      "COMPARISON - Stemming vs Lemmatization:\n",
      "Word            Stemmed         Lemmatized     \n",
      "---------------------------------------------\n",
      "the             the             the            \n",
      "runners         runner          runners        \n",
      "were            were            be             \n",
      "running         run             run            \n",
      "quickly         quickli         quickly        \n",
      "through         through         through        \n",
      "the             the             the            \n",
      "beautiful       beauti          beautiful      \n",
      "gardens         garden          garden         \n",
      "carrying        carri           carry          \n",
      "heavy           heavi           heavy          \n",
      "boxes           box             box            \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"The runners were running quickly through the beautiful gardens, carrying heavy boxes.\"\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Step 1: TOKENIZATION - Split text into individual words\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(\"1. TOKENIZATION:\")\n",
    "print(tokens)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Step 2a: STEMMING - Crude chopping to get word root\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(\"2a. STEMMING (Porter Stemmer):\")\n",
    "print(stemmed_words)\n",
    "print(\"\\nExamples:\")\n",
    "print(f\"  running → {stemmer.stem('running')}\")\n",
    "print(f\"  runner → {stemmer.stem('runner')}\")\n",
    "print(f\"  beautiful → {stemmer.stem('beautiful')}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Step 2b: LEMMATIZATION - Intelligent reduction to dictionary form\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "print(\"2b. LEMMATIZATION:\")\n",
    "print(lemmatized_words)\n",
    "print(\"\\nExamples:\")\n",
    "print(f\"  running → {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "print(f\"  runner → {lemmatizer.lemmatize('runner', pos='n')}\")\n",
    "print(f\"  beautiful → {lemmatizer.lemmatize('beautiful', pos='a')}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# COMPARISON TABLE\n",
    "print(\"COMPARISON - Stemming vs Lemmatization:\")\n",
    "print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for token in tokens:\n",
    "    if token.isalpha():  # Only process words\n",
    "        stemmed = stemmer.stem(token)\n",
    "        lemmatized = lemmatizer.lemmatize(token, pos='v')\n",
    "        print(f\"{token:<15} {stemmed:<15} {lemmatized:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9af66",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487786c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"chat chat chien\",\n",
    "    \"chat souris\",\n",
    "    \"chien souris\",\n",
    "    \"chien souris chien\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
